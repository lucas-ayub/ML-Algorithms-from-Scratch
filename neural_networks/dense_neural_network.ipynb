{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(z, function):\n",
    "    \"\"\"\n",
    "    Computes the activation function for the input.\n",
    "\n",
    "    :param z: The input value or array.\n",
    "    :type z: np.ndarray\n",
    "    :param function: The type of activation function ('relu' or 'sigmoid').\n",
    "    :type function: str\n",
    "    :return: The result of the activation function applied to the input.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    if function == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    elif function == 'sigmoid':\n",
    "        z = np.clip(z, -500, 500)  # To prevent overflow in the exponential function\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function\")\n",
    "\n",
    "def activation_function_derivative(z, function):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the activation function for the input.\n",
    "\n",
    "    :param z: The input value or array.\n",
    "    :type z: np.ndarray\n",
    "    :param function: The type of activation function ('relu' or 'sigmoid').\n",
    "    :type function: str\n",
    "    :return: The derivative of the activation function applied to the input.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    if function == 'relu':\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    elif function == 'sigmoid':\n",
    "        sig = 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # To prevent overflow\n",
    "        return sig * (1 - sig)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X):\n",
    "    \"\"\"\n",
    "    Normalizes the input data by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "    :param X: The input data.\n",
    "    :type X: np.ndarray\n",
    "    :return: The normalized data.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    \n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a single layer in a neural network.\n",
    "    \"\"\"\n",
    "    def __init__ (self, activation_function, num_neurons, num_neurons_in_previous_layer):\n",
    "        \"\"\"\n",
    "        Initializes the layer with random weights and zero biases.\n",
    "\n",
    "        :param activation_function: The activation function used in this layer.\n",
    "        :type activation_function: str\n",
    "        :param num_neurons: The number of neurons in this layer.\n",
    "        :type num_neurons: int\n",
    "        :param num_neurons_in_previous_layer: The number of neurons in the previous layer.\n",
    "        :type num_neurons_in_previous_layer: int\n",
    "        \"\"\"\n",
    "        if activation_function == 'relu':\n",
    "            self.weights = np.random.randn(num_neurons, num_neurons_in_previous_layer) * np.sqrt(2 / num_neurons_in_previous_layer)\n",
    "        else:\n",
    "            self.weights = np.random.randn(num_neurons, num_neurons_in_previous_layer) * 0.01\n",
    "        self.bias = np.zeros((num_neurons, 1))        \n",
    "        self.activation_function = activation_function\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_neurons_in_previous_layer = num_neurons_in_previous_layer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass by computing the linear combination of inputs and weights, adding the bias, and applying the activation function.\n",
    "\n",
    "        :param X: The input data to the layer.\n",
    "        :type X: np.ndarray\n",
    "        :return: The output after applying the activation function.\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        Z = self.weights @ X + self.bias\n",
    "        A = activation_function(Z, self.activation_function)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def update_weights(self, weights, bias):\n",
    "        \"\"\"\n",
    "        Updates the layer's weights and bias.\n",
    "\n",
    "        :param weights: The new weights.\n",
    "        :type weights: np.ndarray\n",
    "        :param bias: The new bias.\n",
    "        :type bias: np.ndarray\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Represents a neural network composed of multiple layers.\n",
    "    \"\"\"\n",
    "    def __init__ (self, layers):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with the provided layers.\n",
    "\n",
    "        :param layers: A list of layers that form the neural network.\n",
    "        :type layers: list of Layer\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        \n",
    "    def optimize(self, X, y, learning_rate, loss_function):\n",
    "        \"\"\"\n",
    "        Optimizes the neural network by performing a forward pass, computing gradients, and updating the weights.\n",
    "\n",
    "        :param X: The input data.\n",
    "        :type X: np.ndarray\n",
    "        :param y: The true labels.\n",
    "        :type y: np.ndarray\n",
    "        :param learning_rate: The learning rate for gradient descent.\n",
    "        :type learning_rate: float\n",
    "        :param loss_function: The loss function to be minimized ('mse', 'cross_entropy', 'binary_cross_entropy').\n",
    "        :type loss_function: str\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        activations = [X]\n",
    "        zs = []  \n",
    "        for layer in self.layers:\n",
    "            Z = layer.weights @ A + layer.bias\n",
    "            A = activation_function(Z, layer.activation_function)\n",
    "            activations.append(A)\n",
    "            zs.append(Z)\n",
    "        \n",
    "        if loss_function == 'mse':\n",
    "            delta = activations[-1] - y  \n",
    "        elif loss_function == 'cross_entropy' or loss_function == 'binary_cross_entropy':\n",
    "            delta = activations[-1] - y  \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "        m = y.shape[1]  \n",
    "        for l in reversed(range(self.num_layers)):\n",
    "            current_layer = self.layers[l]\n",
    "            A_prev = activations[l]\n",
    "            \n",
    "            dW = (1/m) * delta @ A_prev.T\n",
    "            db = (1/m) * np.sum(delta, axis=1, keepdims=True)\n",
    "            current_layer.update_weights(current_layer.weights - learning_rate * dW,\n",
    "                                        current_layer.bias - learning_rate * db)\n",
    "            \n",
    "            if l > 0:\n",
    "                Z = zs[l-1]\n",
    "                delta = (current_layer.weights.T @ delta) * activation_function_derivative(Z, self.layers[l-1].activation_function)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output of the neural network for the given input data.\n",
    "\n",
    "        :param X: The input data.\n",
    "        :type X: np.ndarray\n",
    "        :return: The predicted output.\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "    \n",
    "        Y_hat = A\n",
    "        return Y_hat\n",
    "    \n",
    "    def fit(self, X, y, epochs, batch_size, learning_rate, loss, normalize=False):\n",
    "        \"\"\"\n",
    "        Trains the neural network using the provided training data.\n",
    "\n",
    "        :param X: The input feature matrix.\n",
    "        :type X: np.ndarray\n",
    "        :param y: The true labels.\n",
    "        :type y: np.ndarray\n",
    "        :param epochs: The number of training epochs.\n",
    "        :type epochs: int\n",
    "        :param batch_size: The size of each mini-batch.\n",
    "        :type batch_size: int\n",
    "        :param learning_rate: The learning rate for gradient descent.\n",
    "        :type learning_rate: float\n",
    "        :param loss: The loss function to be minimized ('mse', 'cross_entropy', 'binary_cross_entropy').\n",
    "        :type loss: str\n",
    "        :param normalize: Whether to normalize the input data before training.\n",
    "        :type normalize: bool\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            X = normalize_data(X)\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            indexes = np.random.permutation(X.shape[1]) \n",
    "            X = X[:, indexes]\n",
    "            y = y[:, indexes]\n",
    "            \n",
    "            for i in range(0, X.shape[1], batch_size):\n",
    "                mini_batch_X = X[:, i:i + batch_size]\n",
    "                mini_batch_y = y[:, i:i + batch_size]\n",
    "                self.optimize(mini_batch_X, mini_batch_y, learning_rate, loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "The dataset used for training is the MNIST, it was accesed using tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Flatten the 28x28 images into 784-dimensional vectors\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = np.eye(10)[y_train].T\n",
    "y_test = np.eye(10)[y_test].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.51%\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network\n",
    "network = NeuralNetwork([\n",
    "    Layer('relu', num_neurons=128, num_neurons_in_previous_layer=784),\n",
    "    Layer('relu', num_neurons=64, num_neurons_in_previous_layer=128),\n",
    "    Layer('sigmoid', num_neurons=10, num_neurons_in_previous_layer=64)\n",
    "])\n",
    "\n",
    "# Train the neural network\n",
    "network.fit(X_train, y_train, epochs=10, batch_size=32, learning_rate=0.01, loss='cross_entropy', normalize=False)\n",
    "\n",
    "# Evaluate the neural network\n",
    "y_pred = network.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=0)\n",
    "y_true_classes = np.argmax(y_test, axis=0)\n",
    "\n",
    "accuracy = np.mean(y_pred_classes == y_true_classes)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
