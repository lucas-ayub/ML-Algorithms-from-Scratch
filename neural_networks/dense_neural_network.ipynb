{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(z, function):\n",
    "    \"\"\"\n",
    "    Computes the activation function for the input.\n",
    "\n",
    "    :param z: The input value or array.\n",
    "    :type z: np.ndarray\n",
    "    :param function: The type of activation function ('relu' or 'sigmoid').\n",
    "    :type function: str\n",
    "    :return: The result of the activation function applied to the input.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    if function == 'relu':\n",
    "        return np.maximum(0, z)\n",
    "    elif function == 'sigmoid':\n",
    "        z = np.clip(z, -500, 500)  # To prevent overflow in the exponential function\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function\")\n",
    "\n",
    "def activation_function_derivative(z, function):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the activation function for the input.\n",
    "\n",
    "    :param z: The input value or array.\n",
    "    :type z: np.ndarray\n",
    "    :param function: The type of activation function ('relu' or 'sigmoid').\n",
    "    :type function: str\n",
    "    :return: The derivative of the activation function applied to the input.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    if function == 'relu':\n",
    "        return np.where(z > 0, 1, 0)\n",
    "    elif function == 'sigmoid':\n",
    "        sig = 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # To prevent overflow\n",
    "        return sig * (1 - sig)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported activation function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X):\n",
    "    \"\"\"\n",
    "    Normalizes the input data by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "    :param X: The input data.\n",
    "    :type X: np.ndarray\n",
    "    :return: The normalized data.\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    \n",
    "    return X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a single layer in a neural network.\n",
    "    \"\"\"\n",
    "    def __init__ (self, activation_function, num_neurons, num_neurons_in_previous_layer):\n",
    "        \"\"\"\n",
    "        Initializes the layer with random weights and zero biases.\n",
    "\n",
    "        :param activation_function: The activation function used in this layer.\n",
    "        :type activation_function: str\n",
    "        :param num_neurons: The number of neurons in this layer.\n",
    "        :type num_neurons: int\n",
    "        :param num_neurons_in_previous_layer: The number of neurons in the previous layer.\n",
    "        :type num_neurons_in_previous_layer: int\n",
    "        \"\"\"\n",
    "        if activation_function == 'relu':\n",
    "            self.weights = np.random.randn(num_neurons, num_neurons_in_previous_layer) * np.sqrt(2 / num_neurons_in_previous_layer)\n",
    "        else:\n",
    "            self.weights = np.random.randn(num_neurons, num_neurons_in_previous_layer) * 0.01\n",
    "        self.bias = np.zeros((num_neurons, 1))        \n",
    "        self.activation_function = activation_function\n",
    "        self.num_neurons = num_neurons\n",
    "        self.num_neurons_in_previous_layer = num_neurons_in_previous_layer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass by computing the linear combination of inputs and weights, adding the bias, and applying the activation function.\n",
    "\n",
    "        :param X: The input data to the layer.\n",
    "        :type X: np.ndarray\n",
    "        :return: The output after applying the activation function.\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        Z = self.weights @ X + self.bias\n",
    "        A = activation_function(Z, self.activation_function)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def update_weights(self, weights, bias):\n",
    "        \"\"\"\n",
    "        Updates the layer's weights and bias.\n",
    "\n",
    "        :param weights: The new weights.\n",
    "        :type weights: np.ndarray\n",
    "        :param bias: The new bias.\n",
    "        :type bias: np.ndarray\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Represents a neural network composed of multiple layers.\n",
    "    \"\"\"\n",
    "    def __init__ (self, layers):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with the provided layers.\n",
    "\n",
    "        :param layers: A list of layers that form the neural network.\n",
    "        :type layers: list of Layer\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.num_layers = len(layers)\n",
    "        \n",
    "    def optimize(self, X, y, learning_rate, loss_function):\n",
    "        \"\"\"\n",
    "        Optimizes the neural network by performing a forward pass, computing gradients, and updating the weights.\n",
    "\n",
    "        :param X: The input data.\n",
    "        :type X: np.ndarray\n",
    "        :param y: The true labels.\n",
    "        :type y: np.ndarray\n",
    "        :param learning_rate: The learning rate for gradient descent.\n",
    "        :type learning_rate: float\n",
    "        :param loss_function: The loss function to be minimized ('mse', 'cross_entropy', 'binary_cross_entropy').\n",
    "        :type loss_function: str\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        activations = [X]\n",
    "        zs = []  \n",
    "        for layer in self.layers:\n",
    "            Z = layer.weights @ A + layer.bias\n",
    "            A = activation_function(Z, layer.activation_function)\n",
    "            activations.append(A)\n",
    "            zs.append(Z)\n",
    "        \n",
    "        if loss_function == 'mse':\n",
    "            delta = activations[-1] - y  \n",
    "        elif loss_function == 'cross_entropy' or loss_function == 'binary_cross_entropy':\n",
    "            delta = activations[-1] - y  \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "\n",
    "        m = y.shape[1]  \n",
    "        for l in reversed(range(self.num_layers)):\n",
    "            current_layer = self.layers[l]\n",
    "            A_prev = activations[l]\n",
    "            \n",
    "            dW = (1/m) * delta @ A_prev.T\n",
    "            db = (1/m) * np.sum(delta, axis=1, keepdims=True)\n",
    "            current_layer.update_weights(current_layer.weights - learning_rate * dW,\n",
    "                                        current_layer.bias - learning_rate * db)\n",
    "            \n",
    "            if l > 0:\n",
    "                Z = zs[l-1]\n",
    "                delta = (current_layer.weights.T @ delta) * activation_function_derivative(Z, self.layers[l-1].activation_function)\n",
    "\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the output of the neural network for the given input data.\n",
    "\n",
    "        :param X: The input data.\n",
    "        :type X: np.ndarray\n",
    "        :return: The predicted output.\n",
    "        :rtype: np.ndarray\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "    \n",
    "        Y_hat = A\n",
    "        return Y_hat\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred, loss_function):\n",
    "        \"\"\"\n",
    "        Computes the loss between the true labels and the predicted labels.\n",
    "\n",
    "        :param y_true: The true labels. \n",
    "        :type y_true: np.ndarray\n",
    "        :param y_pred: The predicted labels.\n",
    "        :type y_pred: np.ndarray\n",
    "        :param loss_function: The loss function to be computed ('mse', 'cross_entropy', 'binary_cross_entropy').\n",
    "        :type loss_function: str\n",
    "        :return: The computed loss.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if loss_function == 'mse':\n",
    "            loss = np.mean(np.power(y_pred - y_true, 2))\n",
    "        elif loss_function == 'binary_cross_entropy':\n",
    "            loss = -np.mean(y_true * np.log(y_pred + 1e-10) + (1 - y_true) * np.log(1 - y_pred + 1e-10)) # 1e-10 to prevent log(0) overflow\n",
    "        elif loss_function == 'cross_entropy':\n",
    "            loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-10), axis=1)) # 1e-10 to prevent log(0) overflow\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Computes the accuracy between the true labels and the predicted labels.\n",
    "\n",
    "        :param y_true: The true labels.\n",
    "        :type y_true: np.ndarray\n",
    "        :param y_pred: The predicted labels.\n",
    "        :type y_pred: np.ndarray\n",
    "        :return: The computed accuracy.\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        y_pred_classes = np.argmax(y_pred, axis=0)\n",
    "        y_true_classes = np.argmax(y_true, axis=0)\n",
    "        accuracy = np.mean(y_pred_classes == y_true_classes)\n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, X, y, epochs, batch_size, learning_rate, loss, normalize=False):\n",
    "        \"\"\"\n",
    "        Trains the neural network using the provided training data.\n",
    "\n",
    "        :param X: The input feature matrix.\n",
    "        :type X: np.ndarray\n",
    "        :param y: The true labels.\n",
    "        :type y: np.ndarray\n",
    "        :param epochs: The number of training epochs.\n",
    "        :type epochs: int\n",
    "        :param batch_size: The size of each mini-batch.\n",
    "        :type batch_size: int\n",
    "        :param learning_rate: The learning rate for gradient descent.\n",
    "        :type learning_rate: float\n",
    "        :param loss: The loss function to be minimized ('mse', 'cross_entropy', 'binary_cross_entropy').\n",
    "        :type loss: str\n",
    "        :param normalize: Whether to normalize the input data before training.\n",
    "        :type normalize: bool\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            X = normalize_data(X)\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            indexes = np.random.permutation(X.shape[1]) \n",
    "            X = X[:, indexes]\n",
    "            y = y[:, indexes]\n",
    "            \n",
    "            for i in range(0, X.shape[1], batch_size):\n",
    "                mini_batch_X = X[:, i:i + batch_size]\n",
    "                mini_batch_y = y[:, i:i + batch_size]\n",
    "                self.optimize(mini_batch_X, mini_batch_y, learning_rate, loss)\n",
    "                \n",
    "            y_pred = self.predict(X)\n",
    "            accuracy = self.compute_accuracy(y, y_pred)\n",
    "            loss_value = self.compute_loss(y, y_pred, loss)\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - accuracy: {accuracy:.4f} - loss: {loss_value:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "The dataset used for training is the MNIST, it was accesed using tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "# Flatten the 28x28 images into 784-dimensional vectors\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).T\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = np.eye(10)[y_train].T\n",
    "y_test = np.eye(10)[y_test].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - accuracy: 0.8882 - loss: 0.0788\n",
      "Epoch 2/40 - accuracy: 0.9185 - loss: 0.0564\n",
      "Epoch 3/40 - accuracy: 0.9317 - loss: 0.0459\n",
      "Epoch 4/40 - accuracy: 0.9425 - loss: 0.0397\n",
      "Epoch 5/40 - accuracy: 0.9492 - loss: 0.0352\n",
      "Epoch 6/40 - accuracy: 0.9548 - loss: 0.0314\n",
      "Epoch 7/40 - accuracy: 0.9603 - loss: 0.0281\n",
      "Epoch 8/40 - accuracy: 0.9641 - loss: 0.0259\n",
      "Epoch 9/40 - accuracy: 0.9670 - loss: 0.0237\n",
      "Epoch 10/40 - accuracy: 0.9704 - loss: 0.0221\n",
      "Epoch 11/40 - accuracy: 0.9717 - loss: 0.0206\n",
      "Epoch 12/40 - accuracy: 0.9739 - loss: 0.0194\n",
      "Epoch 13/40 - accuracy: 0.9761 - loss: 0.0181\n",
      "Epoch 14/40 - accuracy: 0.9777 - loss: 0.0173\n",
      "Epoch 15/40 - accuracy: 0.9790 - loss: 0.0165\n",
      "Epoch 16/40 - accuracy: 0.9808 - loss: 0.0155\n",
      "Epoch 17/40 - accuracy: 0.9817 - loss: 0.0146\n",
      "Epoch 18/40 - accuracy: 0.9830 - loss: 0.0138\n",
      "Epoch 19/40 - accuracy: 0.9830 - loss: 0.0138\n",
      "Epoch 20/40 - accuracy: 0.9846 - loss: 0.0127\n",
      "Epoch 21/40 - accuracy: 0.9840 - loss: 0.0128\n",
      "Epoch 22/40 - accuracy: 0.9864 - loss: 0.0115\n",
      "Epoch 23/40 - accuracy: 0.9867 - loss: 0.0114\n",
      "Epoch 24/40 - accuracy: 0.9871 - loss: 0.0109\n",
      "Epoch 25/40 - accuracy: 0.9883 - loss: 0.0101\n",
      "Epoch 26/40 - accuracy: 0.9891 - loss: 0.0096\n",
      "Epoch 27/40 - accuracy: 0.9895 - loss: 0.0096\n",
      "Epoch 28/40 - accuracy: 0.9896 - loss: 0.0093\n",
      "Epoch 29/40 - accuracy: 0.9905 - loss: 0.0087\n",
      "Epoch 30/40 - accuracy: 0.9911 - loss: 0.0084\n",
      "Epoch 31/40 - accuracy: 0.9918 - loss: 0.0079\n",
      "Epoch 32/40 - accuracy: 0.9921 - loss: 0.0077\n",
      "Epoch 33/40 - accuracy: 0.9926 - loss: 0.0073\n",
      "Epoch 34/40 - accuracy: 0.9930 - loss: 0.0070\n",
      "Epoch 35/40 - accuracy: 0.9932 - loss: 0.0068\n",
      "Epoch 36/40 - accuracy: 0.9937 - loss: 0.0067\n",
      "Epoch 37/40 - accuracy: 0.9940 - loss: 0.0064\n",
      "Epoch 38/40 - accuracy: 0.9946 - loss: 0.0059\n",
      "Epoch 39/40 - accuracy: 0.9942 - loss: 0.0060\n",
      "Epoch 40/40 - accuracy: 0.9950 - loss: 0.0057\n",
      "Accuracy: 97.92%\n"
     ]
    }
   ],
   "source": [
    "# Create the neural network\n",
    "network = NeuralNetwork([\n",
    "    Layer('relu', num_neurons=128, num_neurons_in_previous_layer=784),\n",
    "    Layer('relu', num_neurons=64, num_neurons_in_previous_layer=128),\n",
    "    Layer('sigmoid', num_neurons=10, num_neurons_in_previous_layer=64)\n",
    "])\n",
    "\n",
    "# Train the neural network\n",
    "network.fit(X_train, y_train, epochs=40, batch_size=32, learning_rate=0.01, loss='cross_entropy', normalize=False)\n",
    "\n",
    "# Evaluate the neural network\n",
    "y_pred = network.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=0)\n",
    "y_true_classes = np.argmax(y_test, axis=0)\n",
    "\n",
    "accuracy = np.mean(y_pred_classes == y_true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy for test set: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
