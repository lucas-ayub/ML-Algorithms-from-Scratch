{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sigma = 1 / (1 + np.exp(-z))\n",
    "    return sigma\n",
    "\n",
    "def log_loss(y, y_hat):\n",
    "    loss = -(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n",
    "    return loss\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, X, y, epochs, learning_rate = 0.1):\n",
    "        self.num_data = X.shape[0]\n",
    "        self.num_features = X.shape[1]\n",
    "        self.weights = np.random.randn(self.num_features) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def compute_accuracy(self):\n",
    "        correct = 0\n",
    "        for i in range(self.num_data):\n",
    "            x_i = self.X[i]\n",
    "            y_i = self.y[i]\n",
    "            y_hat = self.classify(x_i)\n",
    "            if y_hat == y_i:\n",
    "                correct += 1\n",
    "        return correct / self.num_data\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        loss = 0\n",
    "        for i in range(self.num_data):\n",
    "            x_i = self.X[i]\n",
    "            y_i = self.y[i]\n",
    "            z_i = np.dot(self.weights, x_i) + self.bias\n",
    "            y_hat = sigmoid(z_i)\n",
    "            loss += log_loss(y_i, y_hat)\n",
    "        return loss / self.num_data\n",
    "        \n",
    "    def calculate_gradients(self):\n",
    "        grad_w = np.zeros(self.num_features)\n",
    "        grad_b = 0\n",
    "        for i in range(self.num_data):\n",
    "            x_i = self.X[i]\n",
    "            y_i = self.y[i]\n",
    "            z_i = np.dot(self.weights, x_i) + self.bias\n",
    "            diff = sigmoid(z_i) - y_i\n",
    "            grad_w += diff * x_i\n",
    "            grad_b += diff\n",
    "        grad_w, grad_b = grad_w / self.num_data , grad_b / self.num_data\n",
    "        return grad_w, grad_b\n",
    "        \n",
    "    def update_weights(self):\n",
    "        grad_w, grad_b = self.calculate_gradients()\n",
    "        self.weights -= self.learning_rate * grad_w\n",
    "        self.bias -= self.learning_rate * grad_b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        z = np.dot(self.weights, X) + self.bias\n",
    "        y_hat = sigmoid(z)\n",
    "        return y_hat\n",
    "    \n",
    "    def classify(self, X, threshold = 0.5):\n",
    "        y_hat = self.predict(X)\n",
    "        return int(y_hat > threshold) \n",
    "      \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.update_weights()\n",
    "            loss = self.compute_loss()\n",
    "            accuracy = self.compute_accuracy()\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs} - accuracy: {accuracy:.4f} - loss: {loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
